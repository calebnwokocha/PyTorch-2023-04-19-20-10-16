import math
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.datasets import load_diabetes
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

class Node(nn.Module):
    def __init__(self):
        super(Node, self).__init__()
        self.linear = nn.Linear(10, 1)
        self.w1 = nn.Parameter(torch.randn(1, requires_grad=True))
        self.w2 = nn.Parameter(torch.randn(1, requires_grad=True))

    #The forward function takes a vector of size 10 and returns a scalar value
    def forward(self, x):
        reduced_input = self.linear(x)
        y = ((self.w1 * reduced_input) / (self.w1 + (self.w2 * reduced_input))) * reduced_input
        return y

# Define an optimizer with stochastic gradient descent and learning rate 0.01
node = Node()
optimizer = optim.SGD(node.parameters(), lr=0.01)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.0001, patience=5)

# Load the diabetes dataset, and standardize the input features and target variable
diabetes = load_diabetes()
X = diabetes.data
y = diabetes.target
scaler = StandardScaler()
X = scaler.fit_transform(X)
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32).view(-1, 1)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the node for 10000 epochs
for epoch in range(10000):
    for i in range(len(X)):
        # Zero the gradients to avoid accumulating them
        optimizer.zero_grad()

        # Make a prediction for a single input
        y_pred = node(X[i])

        # Compute the mean squared error loss for a single input
        loss = nn.MSELoss()(y_pred, y[i])

        # Compute gradients by backpropagation
        loss.backward()

        # Update the node parameters
        optimizer.step()

    # Update the learning rate
    scheduler.step(loss)

    if epoch % 100 == 0:
        print(f'Training epoch {epoch}: Loss = {loss.item()}')
